@article{10.1049/el.2019.4202, 
year = {2020}, 
title = {{Music genre classification and music recommendation by using deep learning}}, 
author = {Elbir, A. and Aydin, N.}, 
journal = {Electronics Letters}, 
issn = {0013-5194}, 
doi = {10.1049/el.2019.4202}, 
abstract = {{Today, music is a very important and perhaps inseparable part of people's daily life. There are many genres of music and these genres are different from each other, resulting in people to have different preferences of music. As a result, it is an important and up‐to‐date issue to classify music and to recommend people new music in music listening applications and platforms. Classifying music by their genre is one of the most useful techniques used to solve this problem. There are a number of approaches for music classification and recommendation. One approach is based on the acoustic characteristics of music. In this study, a music genre classification system and music recommendation engine, which focuses on extracting representative features that have been obtained by a novel deep neural network model, have been proposed. Acoustic features extracted from these networks have been utilised for music genre classification and music recommendation on a data set.}}, 
pages = {627--629}, 
number = {12}, 
volume = {56}, 
local-url = {file://localhost/C:\Users\user\Documents\_Uni\ELEC5305%20Acoustics\Scripts\Something%20Awesome\References\Elbir%202020.pdf}
}
@article{10.1080/09298215.2018.1438476, 
year = {2018}, 
title = {{Ensemble of deep learning, visual and acoustic features for music genre classification}}, 
author = {Nanni, Loris and Costa, Yandre M G and Aguiar, Rafael L and Silla, Carlos N and Brahnam, Sheryl}, 
journal = {Journal of New Music Research}, 
issn = {0929-8215}, 
doi = {10.1080/09298215.2018.1438476}, 
pages = {383--397}, 
number = {4}, 
volume = {47}, 
local-url = {file://localhost/C:\Users\user\Documents\_Uni\ELEC5305%20Acoustics\Scripts\Something%20Awesome\References\Nanni%202018.pdf}
}
@article{10.5334/tismir.10, 
year = {2018}, 
title = {{Multimodal Deep Learning for Music Genre Classification}}, 
author = {Oramas, Sergio and Barbieri, Francesco and Nieto, Oriol and Serra, Xavier}, 
journal = {Transactions of the International Society for Music Information Retrieval}, 
issn = {2514-3298}, 
doi = {10.5334/tismir.10}, 
abstract = {{Music genre labels are useful to organize songs, albums, and artists into broader groups that share similar musical characteristics. In this work, an approach to learn and combine multimodal data representations for music genre classification is proposed. Intermediate representations of deep neural networks are learned from audio tracks, text reviews, and cover art images, and further combined for classification. Experiments on single and multi-label genre classification are then carried out, evaluating the effect of the different learned representations and their combinations. Results on both experiments show how the aggregation of learned representations from different modalities improves the accuracy of the classification, suggesting that different modalities embed complementary information. In addition, the learning of a multimodal feature space increases the performance of pure audio representations, which may be specially relevant when the other modalities are available for training, but not at prediction time. Moreover, a proposed approach for dimensionality reduction of target labels yields major improvements in multi-label classification not only in terms of accuracy, but also in terms of the diversity of the predicted genres, which implies a more fine-grained categorization. Finally, a qualitative analysis of the results sheds some light on the behavior of the different modalities on the classification task.}}, 
pages = {4--21}, 
number = {1}, 
volume = {1}, 
local-url = {file://localhost/C:\Users\user\Documents\_Uni\ELEC5305%20Acoustics\Scripts\Something%20Awesome\References\Oramas%202018.pdf}
}
@article{undefined, 
year = {2021}, 
title = {{Spectral Roll-off Points Variations: Exploring Useful Information in Feature Maps by Its Variations}}, 
author = {Yu, Yunkai and You, Yuyang and Yang, Zhihong and Liu, Guozheng and Li, Peiyao and Yang, Zhicheng and Shan, Wenjing}, 
journal = {arXiv}, 
eprint = {2102.00369}, 
abstract = {{Useful information (UI) is an elusive concept in neural networks. A quantitative measurement of UI is absent, despite the variations of UI can be recognized by prior knowledge. The communication bandwidth of feature maps decreases after downscaling operations, but UI flows smoothly after training due to lower Nyquist frequency. Inspired by the low-Nyqusit-frequency nature of UI, we propose the use of spectral roll-off points (SROPs) to estimate UI on variations. The computation of an SROP is extended from a 1-D signal to a 2-D image by the required rotation invariance in image classification tasks. SROP statistics across feature maps are implemented as layer-wise useful information estimates. We design sanity checks to explore SROP variations when UI variations are produced by variations in model input, model architecture and training stages. The variations of SROP is synchronizes with UI variations in various randomized and sufficiently trained model structures. Therefore, SROP variations is an accurate and convenient sign of UI variations, which promotes the explainability of data representations with respect to frequency-domain knowledge.}}, 
local-url = {file://localhost/C:\Users\user\Documents\_Uni\ELEC5305%20Acoustics\Scripts\Something%20Awesome\References\Yu%202021.pdf}
}